{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepEEG import input_preparation\n",
    "from DeepEEG.input_preparation import get_filepaths\n",
    "from DeepEEG.input_preparation import get_labels\n",
    "from DeepEEG.input_preparation import get_data\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the names of the columns\n",
    "#column_names = ['EEG1', 'EEG2', 'Acc_X', 'Acc_Y', 'Acc_Z']\n",
    "\n",
    "# Read the CSV file with specified column names\n",
    "#Data = pd.read_csv(\"Data/Reading/r 1cleaned.csv\",\n",
    "#                    usecols=[0, 1, 2, 3, 4],\n",
    "#                      names=column_names)\n",
    "\n",
    "# Now, 'Data' is a DataFrame with the specified column names\n",
    "#Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"Data\"\n",
    "training_filepaths = get_filepaths(root_folder)\n",
    "labels = get_labels(root_folder)\n",
    "#training_filepaths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(filepaths):\n",
    "    list = []\n",
    "    for i in filepaths.keys():\n",
    "        list.append(i)\n",
    "    return(list)\n",
    "\n",
    "\n",
    "def get_list_with_index(list_data_dict):\n",
    "    for i, data_dict in enumerate(list_data_dict):\n",
    "        print(f\"Index: {i}, Dictionary: {data_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_dict = get_list(training_filepaths)\n",
    "\n",
    "#list_with_index = get_list_with_index(list_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasignals_s_1, one_hot_s_1, label_s_1 = get_data(\n",
    "list_data_dict[0],\n",
    "labels, training_filepaths)\n",
    "# Assuming user_number is the user number \n",
    "# you want to add to datasignals_s_1\n",
    "user_numbers = [1,2,3,4,5,6]\n",
    "\n",
    "\n",
    "\n",
    "datasignals_s_2, one_hot_s_2, label_s_2 = get_data(\n",
    "list_data_dict[3],labels, training_filepaths)\n",
    "\n",
    "\n",
    "datasignals_s_3, one_hot_s_3, label_s_3 = get_data(\n",
    "list_data_dict[1],labels, training_filepaths)\n",
    "datasignals_s_4, one_hot_s_4, label_s_4 = get_data(\n",
    "list_data_dict[2],labels, training_filepaths)\n",
    "datasignals_r_1, one_hot_r_1, label_r_1 = get_data(\n",
    "list_data_dict[8],\n",
    "labels, training_filepaths)\n",
    "\n",
    "datasignals_r_2, one_hot_r_2, label_r_2 = get_data(\n",
    "list_data_dict[9],labels, training_filepaths)\n",
    "datasignals_r_3, one_hot_r_3, label_r_3 = get_data(\n",
    "list_data_dict[5],labels, training_filepaths)\n",
    "datasignals_r_4, one_hot_r_4, label_r_4 = get_data(\n",
    "list_data_dict[6],labels, training_filepaths)\n",
    "datasignals_r_5, one_hot_r_5, label_r_5 = get_data(\n",
    "list_data_dict[4],labels, training_filepaths)\n",
    "datasignals_r_6, one_hot_r_6, label_r_6 = get_data(\n",
    "list_data_dict[7],labels, training_filepaths)\n",
    "\n",
    "\n",
    "datasignals_w_1, one_hot_w_1, label_w_1 = get_data(\n",
    "list_data_dict[10],labels, training_filepaths)\n",
    "datasignals_w_2, one_hot_w_2, label_w_2 = get_data(\n",
    "list_data_dict[12],labels, training_filepaths)\n",
    "\n",
    "datasignals_w_3, one_hot_w_3, label_w_3 = get_data(\n",
    "list_data_dict[14],labels, training_filepaths)\n",
    "\n",
    "datasignals_w_4, one_hot_w_4, label_w_4 = get_data(\n",
    "list_data_dict[13],labels, training_filepaths)\n",
    "\n",
    "datasignals_w_5, one_hot_w_5, label_w_5 = get_data(\n",
    "list_data_dict[11],labels, training_filepaths)\n",
    "\n",
    "datasignals_w_6, one_hot_w_6, label_w_6 = get_data(\n",
    "list_data_dict[15],labels, training_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_to_dataframe(dataframe, column_name, values):\n",
    "    \"\"\"\n",
    "    Add a new column to a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - dataframe: The pandas DataFrame to which the \n",
    "    column will be added.\n",
    "    - column_name: Name of the new column.\n",
    "    - values: Values to be assigned to the new column.\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with the new column.\n",
    "    \"\"\"\n",
    "    dataframe = dataframe.assign(**{column_name: values})\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "datasignals_s_1 = add_column_to_dataframe(datasignals_s_1, \n",
    "'User_Number', user_numbers[0])\n",
    "datasignals_s_2 = add_column_to_dataframe(datasignals_s_2, \n",
    " 'User_Number', user_numbers[1])\n",
    "datasignals_s_3 = add_column_to_dataframe(datasignals_s_3, \n",
    " 'User_Number', user_numbers[2])\n",
    "datasignals_s_4 = add_column_to_dataframe(datasignals_s_4, \n",
    "'User_Number', user_numbers[3])\n",
    "datasignals_r_1 = add_column_to_dataframe(datasignals_r_1, \n",
    "'User_Number',user_numbers[0])\n",
    "datasignals_r_2 = add_column_to_dataframe(datasignals_r_2, \n",
    "'User_Number', user_numbers[1])\n",
    "datasignals_r_3 = add_column_to_dataframe(datasignals_r_3, \n",
    "'User_Number', user_numbers[2])\n",
    "datasignals_r_4 = add_column_to_dataframe(datasignals_r_4, \n",
    "'User_Number', user_numbers[3])\n",
    "datasignals_r_5 = add_column_to_dataframe(datasignals_r_5, \n",
    "'User_Number', user_numbers[4])\n",
    "datasignals_r_6 = add_column_to_dataframe(datasignals_r_6,\n",
    "'User_Number', user_numbers[5])\n",
    "datasignals_w_1 = add_column_to_dataframe(datasignals_w_1, \n",
    " 'User_Number', user_numbers[0])\n",
    "datasignals_w_2 = add_column_to_dataframe(datasignals_w_2,\n",
    "'User_Number', user_numbers[1])\n",
    "datasignals_w_3 = add_column_to_dataframe(datasignals_w_3,\n",
    " 'User_Number', user_numbers[2])\n",
    "datasignals_w_4 = add_column_to_dataframe(datasignals_w_4, \n",
    "'User_Number', user_numbers[3])\n",
    "datasignals_w_5 = add_column_to_dataframe(datasignals_w_5,\n",
    "'User_Number', user_numbers[4])\n",
    "datasignals_w_6 = add_column_to_dataframe(datasignals_w_6,\n",
    "'User_Number', user_numbers[5] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_data_s = pd.concat([datasignals_s_1,\n",
    "datasignals_s_2,datasignals_s_3,datasignals_s_4], axis=0, \n",
    "          ignore_index=True)\n",
    "\n",
    "concatenated_data_r = pd.concat([ datasignals_r_1,\n",
    "datasignals_r_2, datasignals_r_3,datasignals_r_4,\n",
    "datasignals_r_5, datasignals_r_6], axis=0,\n",
    "ignore_index=True)\n",
    "\n",
    "concatenated_data_w = pd.concat([\n",
    "    datasignals_w_1, datasignals_w_2, datasignals_w_3,\n",
    "    datasignals_w_4, datasignals_w_5, datasignals_w_6\n",
    "], axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Data_connected = pd.concat([\n",
    "concatenated_data_s, concatenated_data_r, concatenated_data_w\n",
    "],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_activity_data(activity, file_prefix, start_file, end_file):\n",
    "    data_list = []\n",
    "    for i in range(start_file, end_file + 1):\n",
    "        file_path = f\"Data/{activity}/{file_prefix} {i}cleaned.csv\"\n",
    "        data = np.genfromtxt(file_path, delimiter=',')\n",
    "        data_list.append(data)\n",
    "    return np.concatenate(data_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef load_activity_data(activity, file_prefix, num_files):\\n    data_list = []\\n    for i in range(1, num_files + 1):\\n        file_path = f\"Data/{activity}/{file_prefix} {i}cleaned.csv\"\\n        data = np.genfromtxt(file_path, delimiter=\\',\\')\\n        data_list.append(data)\\n    return np.concatenate(data_list, axis=0)\\n\\n# Load data for each activity\\nspeaking_data = load_activity_data(\"Speaking\", \"s\", 4)\\nreading_data = load_activity_data(\"Reading\", \"r\", 6)\\nwatching_data = load_activity_data(\"Watching\", \"w\", 6)\\n\\n# Create labels for each activity\\n\\n# 0 represents speaking speaking_labels\\nspeaking_labels = np.full((speaking_data.shape[0],), fill_value=0) \\n# 1 represents reading  \\nreading_labels = np.full((reading_data.shape[0],), fill_value=1)\\n# 2 represents watching\\nwatching_labels = np.full((watching_data.shape[0],), fill_value=2)  \\n\\n# Concatenate data and labels reading_data speaking_labels\\nX_array = np.concatenate((speaking_data, \\n                    reading_data, watching_data), axis=0)\\ny_array = np.concatenate((speaking_labels, \\n                    reading_labels, watching_labels), axis=0)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "def load_activity_data(activity, file_prefix, num_files):\n",
    "    data_list = []\n",
    "    for i in range(1, num_files + 1):\n",
    "        file_path = f\"Data/{activity}/{file_prefix} {i}cleaned.csv\"\n",
    "        data = np.genfromtxt(file_path, delimiter=',')\n",
    "        data_list.append(data)\n",
    "    return np.concatenate(data_list, axis=0)\n",
    "\n",
    "# Load data for each activity\n",
    "speaking_data = load_activity_data(\"Speaking\", \"s\", 4)\n",
    "reading_data = load_activity_data(\"Reading\", \"r\", 6)\n",
    "watching_data = load_activity_data(\"Watching\", \"w\", 6)\n",
    "\n",
    "# Create labels for each activity\n",
    "\n",
    "# 0 represents speaking speaking_labels\n",
    "speaking_labels = np.full((speaking_data.shape[0],), fill_value=0) \n",
    "# 1 represents reading  \n",
    "reading_labels = np.full((reading_data.shape[0],), fill_value=1)\n",
    "# 2 represents watching\n",
    "watching_labels = np.full((watching_data.shape[0],), fill_value=2)  \n",
    "\n",
    "# Concatenate data and labels reading_data speaking_labels\n",
    "X_array = np.concatenate((speaking_data, \n",
    "                    reading_data, watching_data), axis=0)\n",
    "y_array = np.concatenate((speaking_labels, \n",
    "                    reading_labels, watching_labels), axis=0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'\\n\\ndef load_activity_data_2(activity, file_prefix, num_files):\\n    data_list = []\\n    for i in range(2, num_files + 1):\\n        file_path = f\"Data/{activity}/{file_prefix} {i}cleaned.csv\"\\n        data = np.genfromtxt(file_path, delimiter=\\',\\')\\n        data_list.append(data)\\n    return np.concatenate(data_list, axis=0)\\n\\n# Load data for each activity\\nreading_data_2 = load_activity_data_2(\"Reading\", \"r\", 6)\\nspeaking_data_2 = load_activity_data_2(\"Speaking\", \"s\", 4)\\nwatching_data_2 = load_activity_data_2(\"Watching\", \"w\", 6)\\n\\n# Create labels for each activity\\n\\n# 0 represents speaking speaking_labels\\nspeaking_labels_2 = np.full((speaking_data_2.shape[0],), fill_value=0) \\n# 1 represents reading  \\nreading_labels_2 = np.full((reading_data_2.shape[0],), fill_value=1)\\n# 2 represents watching\\nwatching_labels_2 = np.full((watching_data_2.shape[0],), fill_value=2)  \\n\\n# Concatenate data and labels reading_data_2\\nX_array_2 = np.concatenate((speaking_data_2, \\n                    reading_data_2, watching_data_2), axis=0)\\ny_array_2 = np.concatenate((speaking_labels_2, \\n                    reading_labels_2, watching_labels_2), axis=0)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''\n",
    "\n",
    "def load_activity_data_2(activity, file_prefix, num_files):\n",
    "    data_list = []\n",
    "    for i in range(2, num_files + 1):\n",
    "        file_path = f\"Data/{activity}/{file_prefix} {i}cleaned.csv\"\n",
    "        data = np.genfromtxt(file_path, delimiter=',')\n",
    "        data_list.append(data)\n",
    "    return np.concatenate(data_list, axis=0)\n",
    "\n",
    "# Load data for each activity\n",
    "reading_data_2 = load_activity_data_2(\"Reading\", \"r\", 6)\n",
    "speaking_data_2 = load_activity_data_2(\"Speaking\", \"s\", 4)\n",
    "watching_data_2 = load_activity_data_2(\"Watching\", \"w\", 6)\n",
    "\n",
    "# Create labels for each activity\n",
    "\n",
    "# 0 represents speaking speaking_labels\n",
    "speaking_labels_2 = np.full((speaking_data_2.shape[0],), fill_value=0) \n",
    "# 1 represents reading  \n",
    "reading_labels_2 = np.full((reading_data_2.shape[0],), fill_value=1)\n",
    "# 2 represents watching\n",
    "watching_labels_2 = np.full((watching_data_2.shape[0],), fill_value=2)  \n",
    "\n",
    "# Concatenate data and labels reading_data_2\n",
    "X_array_2 = np.concatenate((speaking_data_2, \n",
    "                    reading_data_2, watching_data_2), axis=0)\n",
    "y_array_2 = np.concatenate((speaking_labels_2, \n",
    "                    reading_labels_2, watching_labels_2), axis=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_Data_connected.shape, all_Data_connected.columns\n",
    "# all_Data_connected.describe()\n",
    "#datasignals_r_1.to_csv('datasignals_r_1.csv', index=False)\n",
    "#all_Data_connected.to_csv('all_Data_connected.csv'\n",
    "#,index=False)\n",
    "#concatenated_data_s.to_csv('concatenated_data_s.csv'\n",
    "#, index=False)\n",
    "#concatenated_data_r.to_csv('concatenated_data_r.csv', \n",
    "#index=False)\n",
    "#concatenated_data_s.to_csv('concatenated_data_s.csv', index=False)\n",
    "#concatenated_data_w.to_csv('concatenated_data_w.csv', \n",
    "#index=False)\n",
    "#all_Data_connected_3\n",
    "#all_Data_connected_3\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_windows,\n",
    "# y_windows, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list ():\n",
    "    labels_list = []\n",
    "    for key in labels.keys():\n",
    "        labels_list.append(key)\n",
    "    return labels_list\n",
    "\n",
    "\n",
    "\n",
    "labels_list= make_list ()\n",
    "\n",
    "\n",
    "\n",
    "def predicated_label():\n",
    "    predicated_label = []\n",
    "    for i in range (0, len(y_pred)):\n",
    "   \n",
    "     predicated_label.append(\n",
    "        labels_list[y_pred.argmax(axis=1)[i]])\n",
    "    return predicated_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nimport tensorflow as tf\\nfrom tensorflow import optimizers\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\\nfrom tensorflow.keras.layers import Dropout\\nfrom tensorflow.keras import regularizers\\n\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM\\nfrom tensorflow.keras import regularizers\\nfrom tensorflow.keras.layers import BatchNormalization\\nfrom tensorflow.keras.callbacks import LearningRateScheduler\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Conv1D, LSTM,Dropout, Dense, Activation, BatchNormalization, MaxPooling1D\\nfrom tensorflow.keras import optimizers\\nfrom tensorflow.keras.utils import plot_model\\n# Assuming 'X' is your input data of shape \\n#(1044, 100, 6) and 'y' is your corresponding labels\\ntrain_steps_per_epoch = len(X_train)\\nval_steps_per_epoch  = len(X_test)\\n# Define the CNN model\\n\\n\\n\\n\\n\\n\\n\\n# Assuming X_windows and other variables are defined\\n\\n# weight_decay = 1e-4  # Define your weight decay value\\n# Define the model\\n#\\n\\n\\n\\nmodel2 = Sequential()\\n\\n# Convolutional layers\\nmodel2.add(Conv1D(32, 4, strides=1, \\n                 input_shape=(X_train.shape[1], X_train.shape[2])\\n            , padding='same', activation='relu'))\\nmodel2.add(BatchNormalization())\\nmodel2.add(Conv1D(64, 8, strides=4, padding='same', activation='relu'))\\nmodel2.add(BatchNormalization())\\nmodel2.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\\n\\n# LSTM layers\\nmodel2.add(LSTM(128, return_sequences=True, \\n        activation='tanh', recurrent_activation='hard_sigmoid'))\\nmodel2.add(Dropout(0.4))\\nmodel2.add(LSTM(128, return_sequences=False,\\n activation='tanh', \\n recurrent_activation='hard_sigmoid'))\\nmodel2.add(Dropout(0.4))\\n\\n# Output layer\\nmodel2.add(Dense(3, activation='softmax'))\\n\\n# Compile the model\\nmodel2.compile(optimizer=optimizers.Adam(learning_rate=0.01), \\n    loss='sparse_categorical_crossentropy', \\n    metrics=['accuracy'])\\n\\n# Train the model\\nhistory2= model2.fit(X_train, y_train, \\n    epochs=10,\\n    validation_data=(X_test, y_test),\\n    steps_per_epoch=train_steps_per_epoch,\\n    validation_steps=val_steps_per_epoch)  \\n# Adjust validation split as needed\\n\\n\\n\\n\\n\\n# Accessing the history of training\\ntraining_accuracy = history2.history.get('accuracy')     or history.history.get('acc')\\ntraining_loss = history2.history['loss']\\nvalidation_accuracy = history2.history.get('val_accuracy')or history2.history.get('val_acc')\\nvalidation_loss = history2.history.get('val_loss') or history2.history.get('validation_loss')\\n\\n# Plotting accuracy\\nplt.figure(figsize=(10, 5))\\nplt.subplot(1, 2, 1)\\nif training_accuracy:\\n    plt.plot(training_accuracy, \\n             label='Training Accuracy')\\nif validation_accuracy:\\n    plt.plot(validation_accuracy,\\n              label='Validation Accuracy')\\nplt.xlabel('Epochs')\\nplt.ylabel('Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.legend()\\n\\n# Plotting loss\\nplt.subplot(1, 2, 2)\\nplt.plot(training_loss, label='Training Loss')\\nif validation_loss:\\n    plt.plot(validation_loss, label='Validation Loss')\\nplt.xlabel('Epochs')\\nplt.ylabel('Loss')\\nplt.title('Training and Validation Loss')\\nplt.legend()\\n\\nplt.tight_layout()\\nplt.show()\\n\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, \\\n",
    "Flatten, Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, \\\n",
    "MaxPooling1D, Flatten, Dense, Dropout, LSTM\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM,\\\n",
    "Dropout, Dense, Activation, BatchNormalization, MaxPooling1D\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# Assuming 'X' is your input data of shape \n",
    "#(1044, 100, 6) and 'y' is your corresponding labels\n",
    "train_steps_per_epoch = len(X_train)\n",
    "val_steps_per_epoch  = len(X_test)\n",
    "# Define the CNN model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming X_windows and other variables are defined\n",
    "\n",
    "# weight_decay = 1e-4  # Define your weight decay value\n",
    "# Define the model\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model2.add(Conv1D(32, 4, strides=1, \n",
    "                 input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "            , padding='same', activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Conv1D(64, 8, strides=4, padding='same', activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "\n",
    "# LSTM layers\n",
    "model2.add(LSTM(128, return_sequences=True, \n",
    "        activation='tanh', recurrent_activation='hard_sigmoid'))\n",
    "model2.add(Dropout(0.4))\n",
    "model2.add(LSTM(128, return_sequences=False,\n",
    " activation='tanh', \n",
    " recurrent_activation='hard_sigmoid'))\n",
    "model2.add(Dropout(0.4))\n",
    "\n",
    "# Output layer\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(optimizer=optimizers.Adam(learning_rate=0.01), \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history2= model2.fit(X_train, y_train, \n",
    "    epochs=10,\n",
    "    validation_data=(X_test, y_test),\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    validation_steps=val_steps_per_epoch)  \n",
    "# Adjust validation split as needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Accessing the history of training\n",
    "training_accuracy = history2.history.get('accuracy') \\\n",
    "    or history.history.get('acc')\n",
    "training_loss = history2.history['loss']\n",
    "validation_accuracy = history2.history.get('val_accuracy')\\\n",
    "or history2.history.get('val_acc')\n",
    "validation_loss = history2.history.get('val_loss') \\\n",
    "or history2.history.get('validation_loss')\n",
    "\n",
    "# Plotting accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "if training_accuracy:\n",
    "    plt.plot(training_accuracy, \n",
    "             label='Training Accuracy')\n",
    "if validation_accuracy:\n",
    "    plt.plot(validation_accuracy,\n",
    "              label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "if validation_loss:\n",
    "    plt.plot(validation_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nimport tensorflow as tf \\nfrom tensorflow.keras.callbacks import TensorBoard\\nimport datetime\\n\\nregularization_strength = 0.01\\nclip_value = 0.5\\n\\nmodel3 = tf.keras.models.Sequential([\\n    tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)), \\n    tf.keras.layers.Dense(units=128,activation='relu'\\n,kernel_regularizer=tf.keras.regularizers.l2(\\n    regularization_strength)),\\n    tf.keras.layers.Dropout(0.2),\\n    tf.keras.layers.Dense(units=64, activation='relu'),\\n    tf.keras.layers.Dropout(0.2),\\n\\n    tf.keras.layers.Dense(units=3, activation='relu'),\\n    tf.keras.layers.Dense(units=3,activation='softmax')\\n    \\n    ]\\n)\\n\\nmodel3.compile(optimizer=tf.keras.optimizers.Adam(\\n    learning_rate=0.001, clipvalue=clip_value),\\n               loss='sparse_categorical_crossentropy',\\n               metrics=['accuracy'])\\n\\n#model3.fit(X_train,y_train, epochs=20,\\n#        steps_per_epoch=1000,\\n#          validation_data=(X_test,y_test),\\n#          validation_steps=10)\\n\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple feed forward Network \n",
    "#X_train.shape, X_train\n",
    "'''\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "regularization_strength = 0.01\n",
    "clip_value = 0.5\n",
    "\n",
    "model3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)), \n",
    "    tf.keras.layers.Dense(units=128,activation='relu'\n",
    ",kernel_regularizer=tf.keras.regularizers.l2(\n",
    "    regularization_strength)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "    tf.keras.layers.Dense(units=3, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=3,activation='softmax')\n",
    "    \n",
    "    ]\n",
    ")\n",
    "\n",
    "model3.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001, clipvalue=clip_value),\n",
    "               loss='sparse_categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "#model3.fit(X_train,y_train, epochs=20,\n",
    "#        steps_per_epoch=1000,\n",
    "#          validation_data=(X_test,y_test),\n",
    "#          validation_steps=10)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
